{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "\n",
    "from makegif import create_gif\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "train_dict = {}\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print('Using GPU')\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize((28, 28)), transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_type):\n",
    "    X, y = [], []\n",
    "    dataset_path = \"fashion_mnist_images/\" + data_type\n",
    "    classes = os.listdir(dataset_path)\n",
    "    for class_idx, class_name in enumerate(classes):\n",
    "        class_path = os.path.join(dataset_path, class_name)\n",
    "        if os.path.isdir(class_path):\n",
    "            for img in os.listdir(class_path):\n",
    "                img_path = os.path.join(class_path, img)\n",
    "                image = Image.open(img_path).convert('L')\n",
    "                X.append(transform(image))\n",
    "                y.append(int(class_name))\n",
    "    return TensorDataset(torch.stack(X), torch.tensor(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_data(data_type='train')\n",
    "test_dataset = load_data(data_type='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "BATCH_SIZE = 32\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "def train():\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_dict[epoch] = {}\n",
    "        net.train()\n",
    "        for step, (images, labels) in enumerate(tqdm(train_loader)):\n",
    "            train_dict[epoch][step] = {}\n",
    "            X = images.to(device)\n",
    "            y = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(X)\n",
    "            loss = loss_function(outputs, y) \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            for n, layer in enumerate(net.modules()):  \n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    train_dict[epoch][step][n] = {}\n",
    "                    train_dict[epoch][step][n]['weights'] = layer.weight.detach().cpu().clone()\n",
    "                    train_dict[epoch][step][n]['biases'] = layer.bias.detach().cpu().clone()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{EPOCHS}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  \n",
    "        for images, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy of the network on the test images: {accuracy}%')\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_dict.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (images, label) in (test_loader):\n",
    "    print(label[0])\n",
    "    img = images[0]\n",
    "    images = images.to(device)\n",
    "    preds = net(images)\n",
    "    print(f\"PREDICTION: { torch.argmax(preds[0]) }\")\n",
    "    plt.imshow(img.reshape(28, 28))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib tk\n",
    "\n",
    "style.use(\"dark_background\")\n",
    "\n",
    "png_files = []\n",
    "\n",
    "def visualize_layers(sample_num, fig):\n",
    "    for n, (images, label) in enumerate(test_loader):\n",
    "        output_truth = label[sample_num]\n",
    "        input_data = images[sample_num]\n",
    "\n",
    "        images = images.to(device)\n",
    "        outputs = net(images)\n",
    "\n",
    "        prediction = torch.argmax(outputs[sample_num])\n",
    "\n",
    "        classes = { 0: \"T-shirt/top\", \n",
    "                    1: \"Trouser\", \n",
    "                    2: \"Pullover\", \n",
    "                    3: \"Dress\", \n",
    "                    4: \"Coat\", \n",
    "                    5: \"Sandal\", \n",
    "                    6: \"Shirt\", \n",
    "                    7:\"Sneaker\", \n",
    "                    8:\"Bag\", \n",
    "                    9:\"Ankle boot\"\n",
    "        }\n",
    "\n",
    "        if prediction == output_truth:\n",
    "            title_text = f\"[Truth: { classes[int(output_truth)] }]   [Prediction: { classes[int(prediction)] }]\"\n",
    "            fig.suptitle(title_text, fontsize=20, color='g')\n",
    "        else: \n",
    "            title_text = f\"[Truth: { classes[int(output_truth)] }]   [Prediction: { classes[int(prediction)] }]\"\n",
    "            fig.suptitle(title_text, fontsize=20, color='r')\n",
    "\n",
    "\n",
    "        ax0 = plt.subplot2grid((1, 7), (0, 0), rowspan=1, colspan=1)\n",
    "        ax0.set_xlabel('Input Image')\n",
    "        ax1 = plt.subplot2grid((1, 7), (0, 1), rowspan=1, colspan=1)\n",
    "        ax1.set_xticks([])\n",
    "        ax1.set_xlabel('Layer#1')\n",
    "        ax2 = plt.subplot2grid((1, 7), (0, 2), rowspan=1, colspan=1)\n",
    "        ax2.set_xticks([])\n",
    "        ax2.set_xlabel('ReLU')\n",
    "        ax3 = plt.subplot2grid((1, 7), (0, 3), rowspan=1, colspan=1)\n",
    "        ax3.set_xticks([])\n",
    "        ax3.set_xlabel('Layer#2')\n",
    "        ax4 = plt.subplot2grid((1, 7), (0, 4), rowspan=1, colspan=1)\n",
    "        ax4.set_xticks([])\n",
    "        ax4.set_xlabel('ReLU')\n",
    "        ax5 = plt.subplot2grid((1, 7), (0, 5), rowspan=1, colspan=1)\n",
    "        ax5.set_xlabel('Layer#3')\n",
    "        ax6 = plt.subplot2grid((1, 7), (0, 6), rowspan=1, colspan=1)\n",
    "        ax6.set_xlabel('Softmax')\n",
    "\n",
    "        plt.yticks([i for i in range(10)], [classes[i] for i in range(10)])\n",
    "\n",
    "        layers = []\n",
    "        relu = nn.ReLU()\n",
    "\n",
    "        for params in net.parameters():\n",
    "            if len(params.shape) == 2:\n",
    "                layer = params.sum(axis=1)\n",
    "                relu_layer = layer\n",
    "\n",
    "                relu_layer = [relu(relu_layer).detach().cpu().numpy()]\n",
    "                layer = [layer.detach().cpu().numpy()]\n",
    "\n",
    "                layers.append(layer)\n",
    "                layers.append(relu_layer)\n",
    "\n",
    "        ## remove the last relu layer because we will be using softmax\n",
    "        layers = layers[:-1]\n",
    "        ## Add the softmax layer\n",
    "        layers.append([outputs[sample_num].detach().cpu().numpy()])\n",
    "\n",
    "        layer_1 = np.rot90(layers[0], k=3, axes=(0, 1))\n",
    "        layer_2 = np.rot90(layers[1], k=3, axes=(0, 1))\n",
    "        layer_3 = np.rot90(layers[2], k=3, axes=(0, 1))\n",
    "        layer_4 = np.rot90(layers[3], k=3, axes=(0, 1))\n",
    "        layer_5 = np.rot90(layers[4], k=3, axes=(0, 1))\n",
    "        layer_6 = np.rot90(layers[5], k=3, axes=(0, 1))\n",
    "\n",
    "\n",
    "        ax0.imshow(input_data.reshape(28, 28))\n",
    "\n",
    "        ax1.imshow(layer_1, cmap=\"RdYlGn\")\n",
    "        ax2.imshow(layer_2, cmap=\"RdYlGn\")\n",
    "        ax3.imshow(layer_3, cmap=\"RdYlGn\")\n",
    "        ax4.imshow(layer_4, cmap=\"RdYlGn\")\n",
    "        ax5.imshow(layer_5, cmap=\"RdYlGn\")\n",
    "        ax6.imshow(layer_6, cmap=\"RdYlGn\")\n",
    "\n",
    "        ax0.axis(\"off\")\n",
    "\n",
    "        png_file = f\"./images/{n}.png\"\n",
    "        png_files.append(png_file)\n",
    "        plt.savefig(png_file)\n",
    "        \n",
    "    create_gif(png_files, './images/layers.gif')\n",
    "\n",
    "fig = plt.figure(figsize=(14, 7))\n",
    "visualize_layers(0, fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_gif(png_files, './images/layers.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib tk\n",
    "\n",
    "import time\n",
    "\n",
    "style.use(\"dark_background\")\n",
    "\n",
    "png_files = []\n",
    "\n",
    "def make_plots():\n",
    "    with open(\"train_dict.pkl\", \"rb\") as f:\n",
    "        train_dict = pickle.load(f)\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 8))\n",
    "    ax0 = fig.add_subplot(131)\n",
    "    ax1 = fig.add_subplot(132)\n",
    "    ax2 = fig.add_subplot(133)\n",
    "\n",
    "    epochs = 2\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(\"EPOCH\", epoch)\n",
    "        for step in train_dict[epoch]:\n",
    "            if (step%25) == 0:\n",
    "                # time.sleep(4)\n",
    "                print(\"Step\", step)\n",
    "\n",
    "                weights_0 = train_dict[epoch][step][1][\"weights\"]\n",
    "                weights_1 = train_dict[epoch][step][2][\"weights\"]\n",
    "                weights_2 = train_dict[epoch][step][3][\"weights\"]\n",
    "\n",
    "\n",
    "                ax0.imshow(np.rot90(weights_0, k=3, axes=(0, 1)), cmap=\"RdYlGn\")\n",
    "                ax1.imshow(weights_1, cmap=\"RdYlGn\")\n",
    "                ax2.imshow(np.rot90(weights_2, k=3, axes=(0, 1)), cmap=\"RdYlGn\")\n",
    "\n",
    "                fig.suptitle(f\"Fashion MNIST Linear Layers. Epoch: {epoch+1} Step: {step}\", fontsize=20)\n",
    "\n",
    "                ax0.set_title(\"Layer 1 32x784\")\n",
    "                ax1.set_title(\"Layer 2 32x32\")\n",
    "                ax2.set_title(\"Layer 3 10x32\")\n",
    "\n",
    "                ax0.title.set_fontsize(10)\n",
    "                ax1.title.set_fontsize(10)\n",
    "                ax2.title.set_fontsize(10)\n",
    "\n",
    "                # ax0.set_aspect(0.05)\n",
    "                # ax0.set_xlim(0, 784)\n",
    "                # ax0.set_ylim(0, 32)\n",
    "                fig.set_size_inches(12, 12)\n",
    "\n",
    "\n",
    "                # png_file = f\"./images/{epoch}{step}.png\"\n",
    "                # png_files.append(png_file)\n",
    "                # plt.savefig(png_file)\n",
    "\n",
    "\n",
    "                plt.pause(0.01)\n",
    "                ax0.clear()\n",
    "                ax1.clear()\n",
    "                ax2.clear()\n",
    "\n",
    "    plt.show()\n",
    "    # create_gif(png_files, './images/weights.gif')\n",
    "\n",
    "make_plots()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "visual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
